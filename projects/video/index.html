---
layout: default
---

<section class="main-column">  
    <div class="inner">
        <div class="page-title">
            <h5>Case Study</h5>
            <h1>Annotate Video up to 100x Faster</h1>
            <div class="project-tags"><span>Product Design</span><span>UX Research</span></div>
        </div>
        
        <p class="callout">The goal of this project was to help data scientists track and annotate objects in video files to use as training data for machine learning models.</p>

        <video width="960" autoplay loop muted>
            <source src="/assets/video/elephant.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>

        <h3>The Problem</h3>
        
        <p>Figure Eight users come to the platform with raw data and, through annotations made by our contributor crowd, turn their data into meaningful information. Prior to this project, this raw data included text and images. The problem we were tring to solve was that users could not yet use our platform to annotate video data.</p>

        <p>As an example, a customer who wants to annotate video data may come to us with 100 hours of roadside footage that includes pedestrians, vehicles, important signs, and other significant objects to track. Our marketplace platform allows crowds of annotators to label and track these specific objects in the video. These annotations are returned to the customer to train a model that can detect the objects in future footage.</p>

        <div class="layout-treatment extended-image">
            <img src="/assets/video/data_flow.jpg" />
            <span class="caption">Raw video data gets sent directly to humans who annotate and provide coordinates for the objects in the video.</span>
        </div>

        <h3>Roles/Team</h3>
        
        <p>I was the lead product designer for this project. I worked closely with a project manager, a machine learning scientist, our front-end team, and members of the customer success team who helped us identify sponsored users for the new feature.</p>

        <p>In addition to UI/UX and visual design, I played a significant role in research, user testing, and rapid code prototyping. Our machine learning scientist and I created a fully functional code prototype that was tested with users and iteratively developed as we learned more about what we were creating.</p>

        <h3>Design Process</h3>

        <h4>Research</h4>

        <p>We kicked off the project by talking to customers and prospects to select sponsored users. This ultimately helped us to develop initial user stories, product requirements, and initial wireframe sketches.</p>

        <p>The users for this new feature would be data scientists and contributors (those who annotate raw data). Throughout the design process we referenced four persona types that had previously been created.</p>

        <div class="layout-treatment extended-image">
            <img src="/assets/video/video_personas.jpg" />
            <span class="caption">Figure Eight Customer Personas</span>
        </div>

        <p>We talked to our data scientist customers about the specific video data they had, the kinds of objects needing to be annotated, specific requirements for accuracy, and how annotation information should be formatted and received. We asked what their existing challenges were and about the value a more efficient way to complete their work could bring them. This helped us to create many of our success metrics.</p>

        <p>Equally important to this new feature, we talked to contributors about the kinds of work they find interesting, their obstacles to focusing, and the value our platform brings to them. We supplemented their answers with platform analytics to better understand the who, what, when, where, and why of the work our contributors complete on the platform.</p>

        <p>We also conducted a remote design thinking session with four contributors located in different parts of the world.</p>

        <div class="layout-treatment extended-image">
            <img src="/assets/video/video_design_thinking_excersize.jpg" />
            <span class="caption">Documentation from a remote design thinking session</span>
        </div>

        <h4>Ideation</h4>

        <p>Concerning a primary flow, our machine learning scientist customers needed a path to bring their video data to us, define the instructions for what kind of information they wanted to construe from the data, and toggle settings to create and launch a job with their data to put it in front of contributors. </p>

        <p>A crucial component of this new offering would be a modular tool to annotate video. This would be a highly specialized tool with interactions more commonly seen in drawing tools, such as Photoshop or Sketch.</p>

        <div class="layout-treatment half-width">
            <div>
                    <img src="/assets/video/video_sketch_2b.jpg" />
                    <span class="caption">Translating the needs of the user for video annotation into a modular UI.</span>
            </div>
            <div>
    
                    <img src="/assets/video/video_sketch_2.jpg" />
                    <span class="caption">Understanding toolbar layout for the UI.</span>
            </div>
        </div>

        <h4>Iterative Prototyping and Design</h4>

        <p>Concerning our primary flow, i worked in Sketch and Invision to create low fidelity wireframes. As we routinely met with sponsored users, these mockups were put in front of the customer to validate design decisions. The mocks were then iterated on and the process repeated.</p>

        <div class="layout-treatment half-width">
            <div>
                    <img src="/assets/video/lo-fi_2.png" />
                    <span class="caption">Lo-Fi mocks, created in Sketch.</span>
            </div>
            <div>
                    <img src="/assets/video/lo-fi_2.png" />
                    <span class="caption">Lo-Fi mocks, created in Sketch.</span>
            </div>
        </div>

        <div class="layout-treatment extended-image">
            <p data-height="450" data-theme-id="light" data-slug-hash="eKmMaZ" data-default-tab="result" data-user="jcloughley" data-pen-title="Ontology Sidebar" class="codepen">See the Pen <a href="https://codepen.io/jcloughley/pen/eKmMaZ/">Ontology Sidebar</a> by Joe C (<a href="https://codepen.io/jcloughley">@jcloughley</a>) on <a href="https://codepen.io">CodePen</a>.</p>
            <script async src="https://static.codepen.io/assets/embed/ei.js"></script>
            <span class="caption">A code prototype for sidebar behavior.</span>
        </div>

        <div class="layout-treatment extended-image">
           <p data-height="300" data-theme-id="light" data-slug-hash="wXBRJV" data-default-tab="result" data-user="jcloughley" data-pen-title="Tooltip Behavior" class="codepen">See the Pen <a href="https://codepen.io/jcloughley/pen/wXBRJV/">Tooltip Behavior</a> by Joe C (<a href="https://codepen.io/jcloughley">@jcloughley</a>) on <a href="https://codepen.io">CodePen</a>.</p>
            <script async src="https://static.codepen.io/assets/embed/ei.js"></script>
            <span class="caption">A code prototype for toolbar tooltip micro-interaction.</span>
        </div>

        <div class="layout-treatment extended-image">
<p class="codepen" data-height="1698" data-theme-id="light" data-default-tab="result" data-user="jcloughley" data-slug-hash="JwdMpW" style="height: 1698px; box-sizing: border-box; display: flex; align-items: center; justify-content: center; border: 2px solid black; margin: 1em 0; padding: 1em;" data-pen-title="Ontology Sidebar">
  <span>See the Pen <a href="https://codepen.io/jcloughley/pen/JwdMpW/">
  Ontology Sidebar</a> by Joe C (<a href="https://codepen.io/jcloughley">@jcloughley</a>)
  on <a href="https://codepen.io">CodePen</a>.</span>
</p>
<script async src="https://static.codepen.io/assets/embed/ei.js"></script>
        </div>

        <p>The team decided a machine learning algorithm could be employed to speed up the process for annotating a video. With the help of a machine learning specialist we created a code prototype that would both be used to develop this algorithm and be used for user testing. We gradually iterated on this prototype as we tested it with contributors. I worked directly within the code of the prototype to iterate on the UI. 

        <h4>Design Handoff, Launch and post-launch</h4>

        <p>When both customer flows and the contributor tool prototype were validated enough for a phase 1 release, I moved the design of each into a high-fidelity visual design phase and worked closely with our front end team to turn our prototypes into production ready code.</p>

        <p>The feature was initially put into the hands of our sponsored users and many of our crowd contributors. We remained in touch with these users to continue improving the experience post-release.</p>

        <div class="layout-treatment extended-image">
            <img src="/assets/video/video_tool_final.png" />
            <span class="caption">First release version of the video annotation tool</span>
        </div>

        <h3>Retrospective and Lessons Learned</h3>

        <h4>Speed vs Cost vs Quality</h4>

        <p>In talking to our sponsored users we quickly realized that raw video data was often lengthy and expectations around tracking accuracy were high. On the other hand, in talking to contributors, we realized that fatigue on this type of work would occur quickly with longer videos and accuracy would suffer. Limiting video length however meant higher costs for the customer. There was no perfect solution to this challenge however, to alleviate this challenge, we designed a solution that used machine learning to assist contributors. We set a reasonable limit to video length, and provided customization on how customers could price their work and set a sliding scale for acceptable accuracy.</p> 

        <div class="layout-treatment reduced-image">
            <img src="/assets/video/speed-cost-quality.png" />
        </div>

        <h4>The importance of User Testing</h4>

        <p>The new tool we created to track objects in video was truly original and it was a challenge to build something with UI patterns and interactions found nowhere else. This is why user testing and iterative design was essential. The initially chosen Machine Learning model to assist contributors was greatly assumed to speed up their work, however with user testing, we realized this was not the case. The Machine Learning model was scrapped and an alternative approach put in place. Not conducting the user testing to challenge our initial assumptions would have had significant consequences.

        <h4>Integrating Machine Learning into Design Experiences.</h4>

        <p>Machine Learning became critical to the success of this feature and an essential part of the user experience. Being one of my first major projects to employ machine learning, I had to quickly understand what it was and how to weave it into the user experience.</p>

        <div class="action-area">
            <div class="project-point prev"><a href="../wyndcroft"><h3><i class="fa fa-arrow-circle-o-left"></i>Previous</h3></a><p>Telling A Story: The Wyndcroft School</p></div>

            <div class="project-point next"><a href="../models"><h3>Next<i class="fa fa-arrow-circle-o-right"></i></h3></a><p>Label your Data, Synthetically</p></div>
        </div> 
    </div>
</section>